{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR2uUqsHx3FB",
    "outputId": "6fd879b1-007a-4294-9581-a919fea338d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 19 15:29:02 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   74C    P0    77W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OWC4bHoHRey0"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hp9MWRTWRMO7",
    "outputId": "cba84393-84cf-4558-f5ff-5ba9dad85813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pretrained/jvc-tokenizer/tokenizer_config.json',\n",
       " 'pretrained/jvc-tokenizer/special_tokens_map.json',\n",
       " 'pretrained/jvc-tokenizer/vocab.json',\n",
       " 'pretrained/jvc-tokenizer/merges.txt',\n",
       " 'pretrained/jvc-tokenizer/added_tokens.json',\n",
       " 'pretrained/jvc-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "path='../input/jvc20k/jvc-20k.csv'\n",
    "dataset = pd.read_csv(path)\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def get_training_corpus(dataset):\n",
    "    for i in range(0, len(dataset[\"data\"]), 1000):\n",
    "        yield dataset[\"data\"][i : i + 1000]\n",
    "\n",
    "training_corpus = get_training_corpus(dataset)\n",
    "\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Add PADDING token\n",
    "\n",
    "tokenizer.save_pretrained(\"pretrained/jvc-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318,
     "referenced_widgets": [
      "61c34421d1f5467e8ad5b6f36166d448",
      "68aee1c4f7dc4f099420b5cc8b294e74",
      "b45dcd4323f84a4895eeb09737c9e156",
      "eed151abee884edfb7e26de930ddb683",
      "d9203f7734634ab1b5dcd5014fe58e39",
      "d325af0b234f44869402e6df5ff5854a",
      "9d301e63ed45485093c29ad6ff5afd5b",
      "848e74b1832b48bfa6190639d7cf3dcb",
      "cd57ddd41eee491d8e2fe072ae8d3b00",
      "e4f0191a70d34e3f87ee3630511fdf14",
      "fde000253564496599c41569d9a24e4c"
     ]
    },
    "id": "OXTQOZ-MRO2y",
    "outputId": "78dc344e-85cf-4965-8b96-20be6870ccf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-829a58f40360df3e\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-829a58f40360df3e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c34421d1f5467e8ad5b6f36166d448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a58f40360df3e/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-bbae37a0187a6827.arrow\n",
      "The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: data, content, id, title, author.\n",
      "***** Running training *****\n",
      "  Num examples = 20910\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 130700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='130700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    34/130700 00:22 < 25:56:15, 1.40 it/s, Epoch 0.01/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, \n",
    "                         GPT2LMHeadModel, \n",
    "                         TrainingArguments, \n",
    "                         Trainer, \n",
    "                         DataCollatorForLanguageModeling)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pretrained/jvc-tokenizer/\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"data\"], truncation=True, max_length=256)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False) # Dynamic Padding\n",
    "\n",
    "# Dataset\n",
    "dataset = load_dataset('csv', data_files='../input/jvc20k/jvc-20k.csv') \n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "\n",
    "# Model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pretrained/gpt2-jvc\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=30, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBvbFDq_9G2E",
    "outputId": "ca3554ed-f61c-4784-eab4-fd8823627813"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./pretrained/gpt2-jvc/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./pretrained/gpt2-jvc/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./pretrained/gpt2-jvc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file pretrained/jvc-tokenizer/added_tokens.json. We won't load it.\n",
      "loading file pretrained/jvc-tokenizer/vocab.json\n",
      "loading file pretrained/jvc-tokenizer/merges.txt\n",
      "loading file pretrained/jvc-tokenizer/tokenizer.json\n",
      "loading file None\n",
      "loading file pretrained/jvc-tokenizer/special_tokens_map.json\n",
      "loading file pretrained/jvc-tokenizer/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_each = 100\n",
    "\n",
    "shitpost = pipeline(\n",
    "    task='text-generation',\n",
    "    model='pretrained/gpt2-jvc',\n",
    "    tokenizer=\"pretrained/gpt2-jvc\",\n",
    "    max_length=100,\n",
    "    num_return_sequences=n_each\n",
    ");\n",
    "\n",
    "seq_init = [\n",
    "    'AYAAAAA ',\n",
    "    'Les golems ',\n",
    "    'Alain Soral ',\n",
    "    'Pourquoi ',\n",
    "    'Comment ',\n",
    "    '[BORDEL] ',\n",
    "    '[ALERTE] ',\n",
    "    '[PHOTO] ',\n",
    "    'Les ',\n",
    "    'Ton patron: ',\n",
    "    'La caissière: ',\n",
    "    'Ces ',\n",
    "    'Les \"mecs\" qui ',\n",
    "    'Ton excuse ',\n",
    "    'Des kheys ',\n",
    "    'Est-ce que ',\n",
    "]\n",
    "\n",
    "topic_list = []\n",
    "\n",
    "for init in tqdm(seq_init):\n",
    "    current = shitpost(init)\n",
    "    topic_list += list(map(lambda x: x['generated_text'],current))\n",
    "        \n",
    "df = pd.DataFrame(topic_list)\n",
    "df.to_csv(f'topic-{n_each*len(seq_init)}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A81M9mPV9cql",
    "outputId": "f3c26f80-216b-4b21-a6e3-9efed7eb3be3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Les golems vous trouvez un forum\\nnan des poirier?\\nAllez hop hop hop hop hop 2 ans\\nLes golems vont sont effet la victoire ou tout faire, peut bien plus de 2 ans, je me montre et l'intérieur pour la mec de notre pouvais tout quelqu'un qui est mal à peut bien pas, c'est en chercher ou presque chose à plus de 2 ans à la mec vous trouvez pas?\\nEn gros ça j'ai pas d'être pas moi vous vous pensez\"},\n",
       " {'generated_text': \"Les golems face à livreur vous jouez le kheys.\\n\\nEn France couille face et de livreur vous jouez le kheys ne pas faire un golems face à livreur vous jouez à votre ville contre un pays? Ca obèse? Ça pour un pays de l'amour l'étranger pour le pays? Ca transgenres? Ça quand les pays de l'époque.\\nJe suis pas les kheys ici est un pays d'une face et tes vous jouez les kheys.\\nD'ailleurs le pays? Ca obèse\"},\n",
       " {'generated_text': \"Les golems : Tiktok, c'est quoi pas, c'est pas quoi à bon-exercice\\nAlors les kheys, et maintenant?\\nPerso j'ai plus de la population est bien et de la perdu. Aujourd'hui, les kheys qui est a la moins de voir?\\nPerso j'ai fais l'avion, c'est pas un gros crypto.\\nPerso j'ai fais l'avion, on va être envie de lui mais quand j'arrive, je fais\"},\n",
       " {'generated_text': 'Les golems : \"le respect des kheys que n\\'aime que les gens au chômage\" :ouch:\\nOn est d\\'accord? je crois que j\\'arrive que je peux ma chambre\\nvous me suis là sans jouer?\\nOn a fini par une bonne pharmacies à l\\'ai remarqué et je dois...\\nJe suis que j\\'avoir pas le cœur de suite.\\nJe suis pas rien...\\nJ\\'ai remarqué et en dois...\\nOn est d\\'accord que t'},\n",
       " {'generated_text': \"Les golems vous faire la vie par un vie par la vie c'est votre vie et vous pensez 5 les moment?\\nC'est quoi cette merde?\\nPour ça en vie par un vie par un jeux 5% de nos jours?\\nPour ça fait pas les gens ça en passe en première chose c'est moi.\\nLa chose vos questions?\\nLa voix est passe à moi\\nPour la vie par la jeux, comment trop un mec qui sont vaccinés on limite bg une série\"},\n",
       " {'generated_text': 'Les golems : \"J\\'adore\" une fille lui-meme?\\nJe me suis seul mais je parle la peu la site de la qualité\"\\nLa fille a un yeux un kheys.\\nC\\'est pas un 11 avec les kheys, j\\'ai pas fait à se prend au moins de 18 ans, tout les temps comme ça de la qualité mais c\\'est le gros seins aussi et ça.\\nOn a un droit les golems qui se trouve ça a rien que j\\'aurais'},\n",
       " {'generated_text': 'Les golems que mon 10/10 pour les hommes\\nDes questions?\\nLes golems ne sont pas prêts\\nMerci prêts\\nLes golems sont un qualité prêts\\nLa fiction prêts\\nLes golems sont une qualité prêts\\n\\nOn découvre le deux journée (go confinements des hommes filmées)\\nOn doit prêts\\nLes golems sont des qualité que je peux demande sur un hommes filmées.\\nLa journée prêts\\nOn découvre je veux fait les hommes filmées. En vrai les hommes filmées (les apprendre the européenne).'},\n",
       " {'generated_text': 'Les golems : \"je viens de baiser ou pas n\\'y a quand même?\\nVous en pensez quoi?\\nEn toute votre pire?\\nAlors pas quoi?\\nJe ne parle pas à la part que je serais elle a vu en plus facile?\\nJ\\'ai rien pas avoir du laisse le photos\\nVous en pensez quoi?\\nEn toute votre pire?\\nMerci pour 3.\\nLe trouver me dit que vous un truc de la peu mais ça me dit que vous n\\''},\n",
       " {'generated_text': 'Les golems : \"Le vaccin rend les truc hors ultime\"\\nJ\\'espère qu\\'un golems ne sont pas prêts pour le truc hors ultime\"\\nL\\'ensemble de deux ans\\nOn avait tard ensemble, elle-même, ils le moment pas encore aujourd\\'hui, on va équipé si c\\'est un truc hors ultime, on a vu pas des main\\nL\\'ensemble est d\\'etre et le main des truc hors ultime, combien\\nVous avez l\\'élite?\\nOn'},\n",
       " {'generated_text': \"Les golems qui vont me saoule que vous dis avait n'y arrive pas avec l'hôpital\\nEt je vois pas me régale ma vie je cherche des 0 tout le cas de ce que je me regarde dans le test de trouver que je m'ont dit qu'un « ça et que ça demande en vaccins comme les vois maintenant et j'ai réussi avec la zone il est possible à faire plus finir le film mais ça fait peut être un vois maintenant en couple? (j'en ai\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shitpost('Les golems')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "61c34421d1f5467e8ad5b6f36166d448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b45dcd4323f84a4895eeb09737c9e156",
       "IPY_MODEL_eed151abee884edfb7e26de930ddb683",
       "IPY_MODEL_d9203f7734634ab1b5dcd5014fe58e39"
      ],
      "layout": "IPY_MODEL_68aee1c4f7dc4f099420b5cc8b294e74"
     }
    },
    "68aee1c4f7dc4f099420b5cc8b294e74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "848e74b1832b48bfa6190639d7cf3dcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d301e63ed45485093c29ad6ff5afd5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b45dcd4323f84a4895eeb09737c9e156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d301e63ed45485093c29ad6ff5afd5b",
      "placeholder": "​",
      "style": "IPY_MODEL_d325af0b234f44869402e6df5ff5854a",
      "value": "100%"
     }
    },
    "cd57ddd41eee491d8e2fe072ae8d3b00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d325af0b234f44869402e6df5ff5854a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9203f7734634ab1b5dcd5014fe58e39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fde000253564496599c41569d9a24e4c",
      "placeholder": "​",
      "style": "IPY_MODEL_e4f0191a70d34e3f87ee3630511fdf14",
      "value": " 1/1 [00:00&lt;00:00, 20.74it/s]"
     }
    },
    "e4f0191a70d34e3f87ee3630511fdf14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eed151abee884edfb7e26de930ddb683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd57ddd41eee491d8e2fe072ae8d3b00",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_848e74b1832b48bfa6190639d7cf3dcb",
      "value": 1
     }
    },
    "fde000253564496599c41569d9a24e4c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
